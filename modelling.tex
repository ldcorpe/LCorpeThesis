\chapter{Signal and background modelling}
\label{chap:model}

The statistical interpretation of the data requires models of the expected \mgg distribution for the signal and background processes in each category. These models must account for the sources of systematic uncertainty which affect the analysis. %The signal here refers to the diphoton events produced by one of the main Higgs production processes \ggH, \VBF, \ttH, and \VH, where the latter can be further split into \WH and \ZH. 
For the signal processes, the \mgg distribution takes the form of a resonant peak around \mH, the width of which is entirely dominated by detector resolution. The construction of the signal model is described in \Sec~\ref{model:sec:signal_model}. By contrast, the \mgg distribution for background events is a falling non-resonant continuum. The data-driven method by which the background model is produced is described in \Sec~\ref{model:sec:background_model}. Finally, the handling of the systematic uncertainties is described in \Sec~\ref{model:sec:systematics}. 

\section{Signal modelling}
\label{model:sec:signal_model}

\subsection{Parametrisation of the signal \mgg distributions}

The signal model is constructed from samples of simulated \Hgg events from each production mode. The events from the \VH mode are further split into \WH and \ZH samples. % are reconstructed, selected and categorised as described in Chapters~\ref{} and ~\ref{}. 
The shape of the \mgg distribution is parametrised separately for each process in each event category. Since the vertex choice affects the shape of the \mgg  distribution, as was mentioned in \Sec~\ref{reco:sec:vertex}, the modelling is done separately for cases where the \RV or \WV  was selected. 

The signal \mgg distributions are all parametrised using a \DCB function~\cite{CrystalBallFunction} summed with an additional Gaussian function sharing the same mean. We refer to this functional form hereafter as a \DCBpG function. %The \CB functional form is used in \HEP to model processes which involve losses, for example in the semiparmetric regression described in Sec~\ref{}, where it is used to model the energy of individual photons. 
The \DCB shape consists of a Gaussian function core and two tails which are described by power laws. The Gaussian core represents the intrinsic width of the distribution being modelled, while the tails model the under- or over-estimation of the value of the parameter of interest. %For example, the \DCB shape is used iin the semiparmetric regression described in \Sec~\ref{} to model photon energy distribution. The lower tail captures the fact that a certain amount of energy of the photon can be lost in the reconstruction process, this leading to a long non-Gaussian tail of events. Ont he other side, the upper tailis motivated theenergy of the photon shower can be over-estimated, for example because of the inclsuio of showers due to \PU.  
The \DCB shape has five parameters: the width and mean of the Gaussian core, the cross-over points to the power laws on each side, and the order of the two power laws. The \DCBpG function therefore has a total of seven parameters. This is in contrast with the functional form used in~\cite{PAS-HIG-16-020}, which was a sum of up to five Gaussian functions, where the number of functions in the sum was tuned for category, leading to up to 

The choice of the \DCBpG functional form to parametrise the signal \mgg distributions was motivated by the fact that the resulting parametrisation of the signal shape gave closer agreement to the simulated distributions than the simple sum of Gaussian function, but with substantially fewer degrees of freedom. The \DCB shape is also better motivated physically than an arbitrary sum of Gaussian functions. 

The values of the parameters of the functioanl form are determined perfoming a \NLL fit of functional for to the simulated \mgg distrubutions. The resulting parametrisations of the \mgg distributions for the \ggH process (with $\mH=125\GeV$) in the inclusive categories are shown in \Fig~\ref{fig:model:functionalform} for both the \DCBpG and sum of Gaussians functional forms. The improvement in the agreement between the distribution and the parametrisation can be seen by comparing the $\chi^2$ value and the number of degrees of freedom. The \DCBpG always gives a better agreement with a smaller numer of degrees of freedom, and this is also true for the other processes and categories.

\begin{figure}[ht!]
\centering
  \subfloat[Functional form: DCB$+1$G]{\shortstack{
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_UntaggedTag_0.pdf} 
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_UntaggedTag_1.pdf} \\
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_UntaggedTag_2.pdf} 
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_UntaggedTag_3.pdf} 
}}\\
  \subfloat[Functional form: Sum of Gaussians]{\shortstack{
\includegraphics[width=0.3\textwidth]{modellingFigures/nGaus/ggh_UntaggedTag_0.pdf} 
\includegraphics[width=0.3\textwidth]{modellingFigures/nGaus/ggh_UntaggedTag_1.pdf}\\  
\includegraphics[width=0.3\textwidth]{modellingFigures/nGaus/ggh_UntaggedTag_2.pdf} 
\includegraphics[width=0.3\textwidth]{modellingFigures/nGaus/ggh_UntaggedTag_3.pdf} 
}}
\caption{The shape of the simulated \mgg distribution (\mH=125\GeV) for the \ggH process for the inclusive categories when parametrised with (a) \DCBpG and (b) a sum of Gaussian functions, where the RV and WV contributions have been summed according to their relatove fraction. The plots show the agreement between the simulation and the parametrisation expressed as the $\chi^2$, alongside the total number of degrees of freedom in the parametrisation.}

\label{fig:model:functionalform}
\end{figure}

%A number of checks were performed to validate the use of the \DCBpG shape
%insert tests of different functional forms / appendix ?
%tests on DCB response to signal systematics / appendix ?
%bias studies involving DCB shape / appendix ?
%If the number of events in the sample after splitting into provess, category adn \RV/WV case is too low, it can be extremnely difficult to meaningully fit the \DCB1G shape. In this case, a replacement shape from a related high-statistics category is used to model this tag/process.

\subsection{Dependence of model on \mH}

Since the mass of the Higgs boson is a parameter of interest, the parametrisations from simulated signal samples under different assumptions of the Higgs boson mass are combined to form a single continuous model.  
One option is to perform the fitting procedure described above for each different set of \mH scenarios. The individual parameters of the functional form can then be linearly interpolated from one \mH scenario to the next. This is the approach taken in~\cite{}.

Another option, which is used in this analysis, instead performs a simultaneous fit of all the different \mH samples, where the individual parameters of the functional form are themselves polynomials \mH. The fitting procedure can then occur for all mass scenarios at once, where the floating parameters of the fit are the coefficients of polynomial form of each parameter of the functional form. The advantage of this method, which is called \SSF, is a reduction in the total number of parameters used to determine the full model. Indeed, in the linear interpolation method, the total number of floating parameters is increased for sample with a new \mH, since it is parameterised separately from the others. In the \SSF method the additional degrees of freedom come only from the chosen order of the polynomial describing the parameters in the main functional form). Furthermore, the \SSF method guarantees a sensible continuous model, where the linear interpolation method can lead to discontinuous or unphysical models. 

The \SSF method in this analysis used the \DCBpG functional form and was applied separately for each process, category and \RV/\WV case, using the \mgg distributions from under seven different \mH scenarios: 120, 123, 124,125, 126, 127 and 130\GeV. The \mgg distributions were first normalised to the same event yield. The \SSF method  was tested with the parameters of the functional form cast as polynomials of order 0, 1 and 2. It was found that there was no substantial improvement in the agreement between the model and the \mgg distributions when using polynomials of order greater than 0. This was determined to be because the floating parameters were not sufficiently constrained by the \mgg distributions to bring any meaningful improvement.

The signal models for the \RV and \WV contributions are combined according to their relative fraction. The fraction of events where the selected vertex was within $1\cm$ in the $z$-direction from the true vertex is evaluated for each \mH sample, and then parametrised as a first order polynomial to get a smoth dependence on \mH. 

As an example, the depence on \mH of the signal model for the \ggH process in each of the analysis categories is shown in \Fig~\ref{}. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_UntaggedTag_0_fmc_interp.pdf} 
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_UntaggedTag_1_fmc_interp.pdf} \\ 
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_UntaggedTag_2_fmc_interp.pdf} 
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_UntaggedTag_3_fmc_interp.pdf} \\
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_VBFTag_0_fmc_interp.pdf} 
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_VBFTag_1_fmc_interp.pdf} \\
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_TTHLeptonicTag_fmc_interp.pdf} 
\includegraphics[width=0.3\textwidth]{modellingFigures/DCBpG/ggh_TTHHadronicTag_fmc_interp.pdf} 
\caption{The \mH-dependence of the signal models for the ggH process for each of the analysis categories is shown. Each curve shows the signal model for a given value of \mH. The contributions from the RV and WV components of each model were summed together according to their relative size, linearly interpolated between the samples for different \mH.}

\label{fig:model:sig_interpolation}
\end{figure}

\subsection{Normalisation of signal models}

The signal models for each process and category must be normalised to match the event yield expected for the \SM after detector acceptance ($A$) and selection efficiency ($\epsilon$) are taken into account. The total number of events from a given process is determined by multiplying the process \crosssection by the \Hgg \BR and the integrated luminosity of the data sample to be analysed.The values for the Higgs production modes and the \Hgg \BR, as well as their dependence on \mH, are taken from \LHCHXSWG. The simulated signal samples are then normalised such that number of events in the sample at generator level matches the number of events predicted by the \SM. After the simulated samples have been reconstructed and the events selected and categorised, the total number of events remaining is related to the expected number by the \effxacc. The \effxacc is evaluated separately for each process and category, and is parametrised in \mH using a polynomial fit to give a continuous model. The final normalisation is given by the \effxacc multiplied by the total number of expected events for the integrated luminosity of the data sample as a function of \mH.

The signal models for different processes can hen be summed together as required to obtain the expected \mgg distribution of signal events for a given analysis category or for all categories combined. \Fig~\ref{} shows the overall \effxacc for all categories combined as a function of \mH.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\textwidth]{modellingFigures/effAcc_vs_mass.pdf} 
\caption{The \effxacc of all categories combined shown as a function of \mH. The ornage band shows the effect of the systematic uncertainties associated with trigger efficiency, photon identification and selection, photon energy scale and resolution and vertex identification.}

\label{fig:model:sig_effxacc}
\end{figure}

The signal model evaluated at $\mH=125\GeV$ for all categories combined, obtained by summing the normalisaed contributions from each process, are shown in \Fig~\ref{fig:model:sig_model_all}, and for each analysis category in \Fig\s~\ref{fig:model:sig_model_per_category_inc} and~\ref{fig:model:sig_model_per_category_exc}.. 
\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\textwidth]{modellingFigures/DCBpG/all.pdf} 

\caption{The signal model for all analysis categories combined for $\mH=125\GeV$, obtained by summing the contributions from each production process according to the \effxacc. The \effSigma value (half the width of the narrowest interval containing 68.3\% of the invariant mass distribution) and the FWHM (the width of the distribution at half of the maximum value) are also shown. }

\label{fig:model:sig_model_all}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.45\textwidth]{modellingFigures/DCBpG/UntaggedTag_0.pdf} 
\includegraphics[width=0.45\textwidth]{modellingFigures/DCBpG/UntaggedTag_1.pdf} \\
\includegraphics[width=0.45\textwidth]{modellingFigures/DCBpG/UntaggedTag_2.pdf} 
\includegraphics[width=0.45\textwidth]{modellingFigures/DCBpG/UntaggedTag_3.pdf}  

\caption{The signal models for the inclusive analysis categories for $\mH=125\GeV$, obtained by summing the contributions from each production process according to the \effxacc. The \effSigma value (half the width of the narrowest interval containing 68.3\% of the invariant mass distribution) and the FWHM (the width of the distribution at half of the maximum value) are also shown.}

\label{fig:model:sig_model_per_category_inc}
\end{figure}
\begin{figure}[ht!]
\centering
\includegraphics[width=0.45\textwidth]{modellingFigures/DCBpG/VBFTag_0.pdf} 
\includegraphics[width=0.45\textwidth]{modellingFigures/DCBpG/VBFTag_1.pdf} \\
\includegraphics[width=0.45\textwidth]{modellingFigures/DCBpG/TTHLeptonicTag.pdf} 
\includegraphics[width=0.45\textwidth]{modellingFigures/DCBpG/TTHHadronicTag.pdf} 

\caption{The signal models for the \VBFTag and \TTHTag analysis categories for $\mH=125\GeV$, obtained by summing the contributions from each production process according to the \effxacc. The \effSigma value (half the width of the narrowest interval containing 68.3\% of the invariant mass distribution) and the FWHM (the width of the distribution at half of the maximum value) are also shown.}

\label{fig:model:sig_model_per_category_exc}
\end{figure}



%\subsection{Handling of photon energy systematics}

%The systematics associa

\section{Background modelling}
\label{model:sec:background_model}
\subsection{The discrete profiling method}

In this analysis, the background model is produced from data using the discrete profiling method~\cite{DiscreteProfiling}, which treats the choice of functional form to describe the background as a discrete nuisance parameter in the likelihood fit to the data. 

The discrete profiling method was introduced because the underlying functional form of a background distribution is generally not known. Several different families of function could in principle be chosen to parametrise the background distributions, all giving acceptable agreement with the data. Within a given family, it is not always clear which order to choose, or how to process the uncertainty associated with a particular choice of functional form.  %An common solution is to make an arbitrary choice of functional form and try to estimate the bias introduced by pikcing that shape, and trying to assign some systematic uncertainty to compensate.
The discrete profiling method seeks to provide an alternative solution to making an arbitrary choice of function. %Since the chosen functional form itself is not of interest, it can be treated as a discrete nuisance parameter and profiled in the likelihood fit.
and provides a natural way to handle of the uncertainty associated with the choice of background parametrisation. 

When extracting the best-fit value of a parameter of interest using a \NLL minimisation, nuisance parameters representing systematic uncertainties are typically profiled: their value is allowed to be varied during the minimisation, and the final value is not of interest. The additional freedom yields a wider \NLL curve, representing the additional uncertainty attributed the nuisance. 
If the value of the nuisance parameter is instead fixed at the best-fit value, the width of the resulting \NLL curve will be narrower narrower but still with minimum at the same place as for the full profiled \NLL curve. This width represents the uncertainty of the measurement without the effect of the fixed nuisance parameter. If the procedure is repeated for a different fixed values of the nuisance parameter, different \NLL curves, not necessarily at the minimum, will be produced. In the limit that many different values of the fixed nuisance parameter are sampled, the minimum envelope of all the fixed-nuisance \NLL curves will converge to the full profiled \NLL. The uncertainty can then be obtained from the envelope as for a usual \NLL curve. This property is true also for discrete nuisance parameters. The procedure is illustrated in \Fig~\ref{fig:model:bkg_envelope}. When parametrising the background distribution, since the functional form itself is not of interest it can be treated as a discrete nuisance parameter and the method as described above can be used to assess the uncertainty associated with the parametrisation of the background. The fact that different functional forms can have different numbers of degrees of freedom is taken into account by adding a penalty term to the \NLL proportional tot he number of parameters in the functional form. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\textwidth]{modellingFigures/envelope_cartoon.pdf} 
\caption{An illustration of construction of the envelope to estimate the effect of a nuisance parameter. The NLL (denoted as $\Lambda$) curve obtained when performing a likelihood scan of parameter of interest $x$ if the nuisance parameter is profiled is shown in black. The NLL curve obtained by fixing the nuisance to the best-fit value is shown in blue. The NLL curves for various fixed values of the nuisance other than the best-fit are shown in red. The minimum envelope of these curves, shown in green, approximates the original NLL curve obtained by profiling the nuisance parameter.~\cite{DiscreteProfiling}.}

\label{fig:model:bkg_envelope}
\end{figure}

A complete set of all analytic functions should be considered for an exact result. In practice, it is only necessary to include a subset of all analytic function which give a good description of the data. In this analysis, the following four families of functions are considered:

\begin{itemize}
\item Sums of exponentials: $$ f_{N}(x)= \sum^{N}_{i=1} p_{2i} e^{p_{2i+1} x} ,$$
\item Sums of polynomials (in the Bernstein basis): $$ f_{N}(x) = \sum^{N}_{i=0} p_{i} b_{(i,N)}, \text{ where } b_{(i,N)}:= \begin{pmatrix} N \\ i \end{pmatrix} x^i (1-x)^{N-i} ,$$
\item Laurent series: $$ f_{N}(x)= \sum^{N}_{i=1} p_{i} x^{-4 + \sum^{i}_{j=1} (-1)^{j} (j-1)},$$
\item Sums of power-law functions: $$ f_{N}(x)= \sum^{N}_{i=1} p_{2i} x^{-p_{2i+1}},$$
\end{itemize}
where for all $k$, the $p_k$ are a set of floating parameters in the fits, and $N$ represents the \emph{order} of a particular function in the family.  

%The representations of the function families are chosen such that their members are nested: a function of a particular order can reproduce the shape of the functions of lower order for a suitable choice of parameter values. It is therefore only necessary to consider a single representative function from each family when applying the discrete profiling method. 
The maximum order of the candidate function from each family is obtained using the following procedure, separately for each analysis category. Starting with the lowest-order function in the family, the parameters of the candidate function are varied to minimize the \NLL with respect to the $m_{\gamma\gamma}$ distribution. A penalty of 0.5 times the number of parameters in the functional form is added to the value of the \NLL to account for differences in the number of degrees of freedom. The same procedure is applied to the function of next-highest order in the family. 
In the limit of large sample size, the difference in the \NLL between functions of successive orders $N$ and $N+1$, $2 \Delta NLL_{N+1} = 2(NLL_{N+1} - NLL_{N})$, is distributed as a $\chi^2$ with $M$ degrees of freedom where, $M$ is the difference in the number of free parameters in the order-$N+1$ function and order-$N$ functions. A p-value is then calculated as:

$$ \text{p-value} = p(2 \Delta NLL > 2 \Delta NLL_{N+1}| \chi^2(M)). $$

If the p-value is less than a predetermined threshold, chosen as $0.05$, the higher order function is supported by the data, otherwise the higher order function is assumed too flexible given the data. In the former case, the next-highest order function is then considered, an so on. Otherwise, the procedure terminates having found the highest-order suitable function.  An additional constraint is applied to remove low order functions which do not fit the data well. The remaining functions from each of the four families are added to the final set of candidate functions to be used in the discrete profiling, which are shown for each category in \Fig\s~\ref{fig:model_bkg)multipdf_inc} and~\ref{fig:model:bkg_multipdf_exc}.

A series of tests demonstrated that this method provides good coverage of the uncertainty associated with the choice of the function and provides an unbiased estimate of the signal strength. These tests are described in detail in~\cite{DiscreteProfilingMethod}. 

\begin{figure}
 \begin{center}
 \subfloat[UntaggedTag 0]{\includegraphics[width=0.48\textwidth]{modellingFigures/multipdf/multipdf_UntaggedTag_0.pdf}}
 \subfloat[UntaggedTag 1]{\includegraphics[width=0.48\textwidth]{modellingFigures/multipdf/multipdf_UntaggedTag_1.pdf}}\\
 \subfloat[UntaggedTag 2]{\includegraphics[width=0.48\textwidth]{modellingFigures/multipdf/multipdf_UntaggedTag_2.pdf}}
 \subfloat[UntaggedTag 3]{\includegraphics[width=0.48\textwidth]{modellingFigures/multipdf/multipdf_UntaggedTag_3.pdf}}\\
 \caption{The set of candidate functions chosen to parametrise the background using the discrete profiling method in the inclusive categories.}
 \label{fig:model_bkg)multipdf_inc}
 \end{center}
\end{figure}

\begin{figure}
 \begin{center}
 \subfloat[VBFTag 0]      {\includegraphics[width=0.48\textwidth]{modellingFigures/multipdf/multipdf_VBFTag_0.pdf}}
 \subfloat[VBFTag 1]      {\includegraphics[width=0.48\textwidth]{modellingFigures/multipdf/multipdf_VBFTag_1.pdf}}\\
 \subfloat[TTHLeptonicTag]{\includegraphics[width=0.48\textwidth]{modellingFigures/multipdf/multipdf_TTHLeptonicTag.pdf}}
 \subfloat[TTHHadronicTag]{\includegraphics[width=0.48\textwidth]{modellingFigures/multipdf/multipdf_TTHHadronicTag.pdf}}\\
 \caption{The set of candidate functions chosen to parametrise the background using the discrete profiling method in the \VBFTag and \TTHTag categories.} 
 \label{fig:model:bkg_multipdf_exc}
 \end{center}
\end{figure}

The expected number of background events per $\GeV$ around $125\GeV$ is shown in \Table~\ref{tab:model:sig_bkg_yields} alongside the expecte number of signal events for each category, which is broken down by the contribution of each production mode. The \effSigma (the width of the smallest widnow containing $68.3\%$ of the distribution) and $\sigma_{HM}$ (the width of the distribution at half of the maximum value) of the signal model for each category are also shown. The expected number of background events is taken in a $\pm 1 \effSigma$ window around $125\GeV$ using the best-fit background function from the set of candidate functions in each category.

 \begin{table}
  \resizebox{\textwidth}{!}{
\input{modellingFigures/SigBgYields.tex}
}
 \caption{ The expected number of signal and background events per category. The \effSigma of the signal model is also provided as an estimate of the $m_{\gamma\gamma}$ resolution in that category. The expected number of background events is quoted per \GeV in a $\pm 1 \effSigma$ window around 125 \GeV.}
 \label{tab:model:sig_bkg_yields}
\end{table}

\section{Systematic uncertainties}
\label{model:sec:systematics}

The systematic uncertainties considered in this analysis pertain to either the signal or background modelling. Since the background model is obtained from data, the only uncertainty to take into account is associated with the choice functional form for the parametrisation. This is handled directly by the discrete profiling method described in \Sec~\ref{}. The uncertainties which affect the signal model are more numerous, and the way that each is handled is described in this chapter. They are implemented differently depending on their effect on the signal model. 

Systematic uncertainties which affect the shape of the \mgg distribution are built directly into the signal models described in \Sec~\ref{}. Most systematics of this type affect the photon energy, and therefore the invariant mass. For systematics of this type, the effect of the variation is propagated to the mean, \effSigma and normalisation of the signal \mgg distribution for each category and signal process. Corresponding nuisance parameters are then inserted which can modify the normalisation, mean and widths of the \DCBpG parametrisation in each category (one exception is the nuisance corresponding to the vertex efficiency, which instead modifies the relative mixing fraction of the \RV and \WV components of the model). The nuisances parameters are then Gaussian-constrained and allowed to be profiled in the \NLL minimisation when making measurements of the parameters of interest. Nuisances of this type are referred to hereafter as \emph{shape nuisances}. 

Uncertainties on selection efficiencies typically do not affect the shape of the \mgg distribution, but do change the final event count (or \emph{yield}) of the signal mode for each category. Uncertainties of this type are implemented as nuisance parameters which scale the yield of each process and category. Depending on the source of the systematic uncertainty, categories can be scaled differently, although all categories will simultaneously either increase or decrease. The corresponding nuisance parameters are profiled in the \NLL minimisation with a \lnN constraint~\cite{}, which cannot give rise to negative yields. Systematics of this type are referred to as \emph{yield nuisances}, which are either symmetric or asymmetric. In the symmetric case, the upward and downward variations of the uncertainty are the same size. In the asymmetric case, the upward and downward variations are of different sizes.

Finally, certain sources of systematic uncertainty affect the categorisation of events. In this case, the variations in such uncertainties can cause events to move from one event category to another, or altogether out of the acceptance of the analysis. These systematic uncertainties are implemented as nuisance parameters which change the relative yield of event categories, and are referred to as \emph{category migration nuisances}. They are implemented analogously to yield nuisances, except that the yield of certain categories will increase while the yield of others must decrease to preserve the overall normalisation. Furthermore, separate nuisances are implemented to account for different types of migration: for instance, between the individual \VBFTag categories on the one hand, and then between all \VBFTag and all \Untagged categories on the other.

\subsection{Theory uncertainties}

\subsubsection{Parton Distribution Functions}

The uncertainty on signal process \crosssection\s due to \PDF uncertainties is modelled in two ways: the overall effect on the signal yields is is modelled with a set of yield nuisances, while the relative variation on the category yields are modelled with a set of category migration nuisances. 

One symmetric yield nuisance for each signal process is included, accounting for both \PDF and \alphaS uncertainties. Each varies the category yields according to the corresponding uncertainty in the process \crosssection, as provided by the \LHCHXSWG recommendation~\ref{}. Namely, the \ggH, \VBF, \WH, \ZH, and \ttH process \crosssection\s are scaled by 3.2\%, 2.1\%, 1.9\%, 1.6\% and 3.6\% respectively. 

The relative yield change is modelled using a series of category migration nuisances. The size of the variations is determined according to the PDF4LHC prescription~\cite{}, by re-weighting individual events according to the NNPDF30 \PDF set. The effect of the variations is normalised by their effect on the overall yield, such that they represent only migrations. The procedure results in 60 uncorrelated symmetric migration nuisances. The largest migrations are of the order of 2\% of category yields, but are typically below 0.4\%.

\subsubsection{Strong force coupling constant}

The effect of uncertainty on the value of the strong force coupling constant \alphaS is modelled as described in the \PDF uncertainty section. The yield nuisances for the \PDF uncertainties also incorporate the uncertainty on \alphaS. Three additional asymmetric category migration nuisances are included, the effect of which is determined according to the PDF4LHC prescription~\cite{} normalised by the overall effect on the signal yield. The largest migrations are of the order of $3.7\%$ of the category yield. 

\subsubsection{QCD scale}
The uncertainty on the scale of the \QCD interaction is parametrised in terms of the renormalisation ($\mu_{Renorm.}$) and factorisation ($\mu_{Fact.}$) scales. The asymmetric yield nuisances corresponding to variations of these parameters are taken direction from the \LHCHXSWG recommendations for \crosssection\s~\cite{}. The size of the effect for \ggH, \VBF, \WH, \ZH and \ttH processes is $+4.6\%/-6.7\%$, $+0.4\%/-0.3\%$, $+0.5\%/-0.7\%$, $+3.8\%/-3.0\%$ and $+5.8\%/-9.2\%$ respectively. Three additional asymmetric category migration nuisances are also included. The size of their effect is estimated at generator level, by varying the values of $\mu_{Renorm.}$ and $\mu_{Fact.}$ by factors of $2$ (upward variation ) or $0.5$ (downward variation). The three category migration nuisance correspond to three scenarios: varying $\mu_{Renorm.}$ up or down while keeping $\mu_{Fact.}$ constant ; varying $\mu_{Fact.}$ up or down while keeping $\mu_{Renorm.}$ constant ; varying both $\mu_{Renorm.}$ and $\mu_{Fact.}$ up or down uniformly. In each case the migrations are found to be of the order of 5-10\% of the category yield.

\subsubsection{\Hgg branching ratio}
The uncertainty on the \Hgg \BR is taken directly from the \LHCHXSWG recommendation~\ref{}. In is implemented as a yield nuisance affecting all analysis categories, and the variation is of 2.08\% on the category yield.

\subsubsection{Underlying event and parton shower}
The uncertainty on the \emph{underlying event} is introduced because different models exist to describe the interactions of quarks and gluons in \pp collision, each giving different predictions. In particular, this affects the modelling of jets. This uncertainty is taken into account as a set of category migration nuisances. The sizes of the variations are obtained from dedicated simulated samples where the parameters of the generators have been tuned differently. The category migrations evaluated are between: \VBFTag 0 and \VBFTag 1 categories (of the order of 7\%); and all \VBFTag categories and all \Untagged categories (of the order of 9\%).

The \emph{parton shower} refers to the emission of \QCD radiation from partons during \pp collisions. As for the underlying event, an uncertainty is introduced because different models and generators give different predictions, in particular in the reconstruction of jets. The resulting uncertainty is modelled analogously to the underlying event uncertainty.  The category migrations evaluated are between: \VBFTag 0 and \VBFTag 1 categories (of the order of 7\%); and all \VBFTag categories and all \Untagged categories (of the order of 9\%).

\subsubsection{Gluon fusion contamination of \VBFTag and \TTHTag categories}

The theoretical prediction for the jet multiplicity in gluon fusion events is unreliable in can cases with large number of jets. This leads to the introduction of an uncertainty on the contamination of \ggH events in other analysis categories which use jets in their selections. 

For \VBFTag categories, the uncertainty is estimated using the Stewart-Tackmann procedure~\cite{StewartTackmann}. This results in a set of category migration nuisances: a migration between \VBFTag categories (at most $39\%$ on category yield) and a migration between \Untagged and \VBFTag categories (at most $10\%$ on category yield); %jetveto

For the \TTHTag categories, this uncertainty is modelled by three yield nuisances:
\begin{itemize}
\item shower modelling uncertainty, which is estimated by comparing the jet multiplicity in data and simulation for $\Ptop \APtop \rightarrow \text{jets}$ events, where the tops quarks both decay to leptons, leading to variations of the order of 45\% on the yield of the \TTHTag categories; 
\item gluon splitting modelling, which is estimated from the observed ratio of \crosssection\s of  $\Ptop \Ptop \Pbottom \Pbottom$ and $\Ptop \Ptop +2\text{jet}$) events in 13\TeV data;
\item an additional nuisance included due to the small size of the simulated samples used in these studies.
\end{itemize}

\subsection{Photon energy uncertainties}

\subsubsection{Photon energy scale and resolution}

After the calibration of the \ECAL described in \Sec~\ref{}, there are still some discrepancies in the photon energy scale and resolution between simulation and data. Since electrons and photons are both reconstructed as \SC\s, the photon energy scale and resolutions discrepancies can be studied using \Zee events where the electrons are reconstructed as photons.
For \SC\s in eight \RNINE and $|\eta|$ classes, the invariant mass distributions in data and simulation are both fitted with a Breit-Wigner (which models the natural shape of the \PZ peak) convoluted with a \CB function (which describes the \ECAL resolution effect and losses due to unrecovered energy from bremsstrahlung). The natural width and pole mass of the $PZ$ boson are fixed to their accepted values~\cite{PDF} in this parametrisation. 

The corrections to the photon energy scale are given by the relative differences between the best-fit means of the \CB in data and simulation, divided by the $\PZ$ boson pole mass, in each bin. The corrections to the photon energy resolution is applied by adding additional smearing terms to the width of the \CB in quadrature. The additional scale and resolution corrections described above each have uncertainties, which are related to choices made for the \Zee event selection and classification, as well as the difference between the final electron and photon energy regression \BDT\s. The uncertainties on the energy scale and resolution are quantified for each photon class, and propagated to the \mgg distribution in each analysis category. This results in a shape nuisance for the photon energy scale in four (high and low \RNINE, each for \EB and \EE), and eight shape nuisances for the photon energy smearing (as for the photon energy scale, but parametrised also as constant and stochastic contributions). 

The size of the systematic uncertainties are of the order of 0.15\% to 0.5\% depending on the photon class. The effect on the mean of the \mgg distribution is at most 0.25\%, while the effect on the \effSigma is at most 20\%, depending on the analysis category.

\subsubsection{Preselection}
The efficiency of the photon preselection is quantified using the \TagAndProbe method described in \Sec~\ref{}, which also provided systematic uncertainties for different photon classes. The systematic uncertainties are propagated to a yield nuisance,
the effect of which is around 4\% on the category yields.

\subsubsection{Per-photon energy resolution}
The uncertainty of the per-photon energy resolution is conservatively evaluated by scaling the output of the \PhoEnergyBdt described in \Sec~\ref{} by $\pm5\%$. This uncertainty is propagated throughout the analysis and modelled as a yield nuisance. The size of the variation is at most 3\% depending on the analysis category.

\subsubsection{Non-uniformity of the light collection}
The uncertainty on the response of the \ECAL crystals depending on their position in \eta, is estimated by xxx. It is modelled separately as shape nuisances for photons in the barrel and in the endcaps. The size of the uncertainty is 0.07\% on the photon energies. The effect is propagated to the \mgg distribution of each category and applied as separate shape nuisances in the \EE and \EB, which vary the mean by up to 0.2\% and the \effSigma by up to 5\%.

\subsubsection{Non-linearity of detector response}
The uncertainty associated with the fact that the \ECAL response is not linear is estimated by comparing boosted \Zee decays in data and simulation. Individual photon energies are affected by up to 0.2\%. The effect is propagated as a shape nuisance, which varies the mean of the \mgg distribution by 0.1\% in each category.

\subsubsection{Modelling of detector response in \Geant}
The imperfect modelling of electromagnetic showers in the detector simulation software \Geant is modelled with a shape nuisance, as it impacts the photon energy scale. The size of the variation was determined with a dedicated simulated sample where the parameters of shower modelling were modified, leading to a small uncertainty on the photon energy scale. The size of the effect is approximately 0.05\% on the mean of the \mgg distribution in each category.

\subsubsection{Modelling of the material budget}
The imperfect modelling of the amount of material between the vertex and the \ECAL affects the simulation of the photon and electron showers. The uncertainty related to this effect is estimated with dedicated samples there the amount of simulated material is uniformly varied by $\pm 5\%$. It is treated as two separate shape nuisances, for \EB and \EE photons separately, which affect the mean of the \mgg distribution by at most 0.1\% (0.03\%)nd the \effSigma by at most 3.5\% (7.6\%) for the \EB (\EE) photons.

\subsubsection{Shower shape corrections}
The uncertainty deriving from the imperfect modelling of shower shape variables is estimated using simulated samples with and without the corrections. This effect is of order 0.064\% on the photon energy scale, and is implemented as four shape nuisances for photons in different $\eta$ and \RNINE classes, which vary the \mgg distribution mean by at most 0.2\% and \effSigma by at most 1.8\%.

\subsection{Diphoton/per event}

\subsubsection{Vertex-finding efficiency}
The uncertainty on the vertex-finding efficiency is determined by comparing the fraction of correctly identified vertices in \Zmumu between data and simulation. The uncertainty is modelled as a shape nuisance which alters the \RV/\WV mixing fraction of each signal model. The variation is of the order of 1.5\% on the \RV fraction..

\subsubsection{Jet energy scale and resolution}
The uncertainties on the jet energy scale are described by migrations nuisances: between \VBFTag 0 and \VBFTag 1 (at most 13.6\% on the category yield); between all \VBF and \Untagged categories (at most 12\%) ; and between \TTHTag and \Untagged categories (at most 15\%).

The nuisances for the jet energy resolution are treated analogously, with migrations of at most 2.2\% between \VBFTag 0 and \VBFTag 1 categories; at most 2.9\% between \VBF and \Untagged categories ; and at most 12.8\% between \TTHTag and \Untagged categories.

\subsubsection{Integrated luminosity}
The uncertainty on the value of the integrated luminosity of the data sample is modelled as a scale nuisance, the size of which is 6.2\% on the yield of all signal processes. 

\subsubsection{Trigger efficiency}
The uncertainty on the trigger efficiency is estimated using a \TagAndProbe method as described in \Sec~\ref{}. The uncertainty is applied yield nuisance parameter of size 0.1\%.

\subsubsection{$\Pbottom$-tagging  efficiency}
The uncertainty on the tagging of $\Pbottom$ jets is evaluated by varying the ratio between the measured b-tagging efficiency in data and simulation within their uncertainty. This is propagated to a yield nuisance, has an effect of the order of 2\% for \TTHTag categories.

\subsubsection{Lepton reconstruction efficiencies}
The uncertainty on the reconstruction of electrons and muons is determined by considering the ration of leptons reconstructed in data and simulation. The result implemented as a yield uncertainty for the \TTHTag categories of the order of 1\%.

\subsubsection{Unmatched \PU}
The uncertainty relating unmatched \PU is described by migrations nuisances: between \VBFTag 0 and \VBFTag 1 (at most 1.2\% on the category yield); and between all \VBF and \Untagged categories (at most 1.3\%).

\subsubsection{Jet width shift}
The uncertainties on \PU rejection using the selection of the jet width are described by a migrations nuisances between all \VBF and \Untagged categories (at most 3.5\%).
